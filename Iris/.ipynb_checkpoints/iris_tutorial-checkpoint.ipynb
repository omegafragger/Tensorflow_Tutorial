{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<BatchDataset shapes: ({SepalLength: (?,), SepalWidth: (?,), PetalLength: (?,), PetalWidth: (?,)}, (?,)), types: ({SepalLength: tf.float64, SepalWidth: tf.float64, PetalLength: tf.float64, PetalWidth: tf.float64}, tf.int64)>\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Tensorflow tutorial for predicting on the IRIS dataset\n",
    "Also contains general methods which will be useful for:\n",
    "\n",
    "* Downloading training and test sets given their URLs.\n",
    "* \"\"\"\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "TRAIN_URL = \"http://download.tensorflow.org/data/iris_training.csv\"\n",
    "TEST_URL = \"http://download.tensorflow.org/data/iris_test.csv\"\n",
    "\n",
    "CSV_COLUMN_NAMES = ['SepalLength', 'SepalWidth',\n",
    "                    'PetalLength', 'PetalWidth', 'Species']\n",
    "SPECIES = ['Sentosa', 'Versicolor', 'Virginica']\n",
    "\n",
    "\n",
    "\"\"\"The following two methods contain functionality for loading and reading\n",
    "data into the training and test sets.\"\"\"\n",
    "\n",
    "def download_if_not_in_cache(train_url, test_url, train_file_name, test_file_name):\n",
    "    \"\"\"Method to download a training and test set given their URLs.\n",
    "    * The method checks if the file already exists in the cache.\n",
    "    * If not it downloads and stores the file in the cache.\n",
    "    * Finally, it returns the path to the file which has been downloaded.\"\"\"\n",
    "    \n",
    "    train_file_path = tf.keras.utils.get_file(train_file_name, train_url)\n",
    "    test_file_path = tf.keras.utils.get_file(test_file_name, test_url)\n",
    "    \n",
    "    return train_file_path, test_file_path\n",
    "\n",
    "\n",
    "def load_data(train_url, test_url):\n",
    "    train_path, test_path = download_if_not_in_cache(train_url, test_url, train_url.split('/')[-1], test_url.split('/')[-1])\n",
    "\n",
    "    train = pd.read_csv(train_path, names = CSV_COLUMN_NAMES, header=0)\n",
    "    train_x, train_y = train, train.pop('Species')\n",
    "    \n",
    "    test = pd.read_csv(test_path, names = CSV_COLUMN_NAMES, header=0)\n",
    "    test_x, test_y = test, test.pop('Species')\n",
    "    \n",
    "    return ((train_x, train_y), (test_x, test_y))\n",
    "\n",
    "\n",
    "def train_input_function(train_x, train_y, batch_size):\n",
    "    \"\"\"Function to produce a dataset type object from the training data produced\n",
    "    by the load_data function above. The dataset type object produced by this method\n",
    "    can be passed to the 'train' function of the estimator (like DNNClassifier)\"\"\"\n",
    "    \n",
    "    # train_x is a DataFrame type object and we are converting it into a dictionary\n",
    "    # train_y is a Series type object <class 'pandas.core.series.Series'>\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((dict(train_x), train_y))\n",
    "    \n",
    "    # Shuffling, Repeating and Batching the dataset\n",
    "    dataset = dataset.shuffle(1000).repeat().batch(batch_size)\n",
    "    \n",
    "    return dataset\n",
    "\n",
    "\n",
    "def test_input_function(test_x, test_y, batch_size):\n",
    "    \"\"\"Similar function as above but for the test data set\"\"\"\n",
    "    features = dict(test_x)\n",
    "    \n",
    "    # This case may turn up when we are trying to make predictions\n",
    "    # on single points using a trained classifier\n",
    "    if test_y is None:\n",
    "        inputs = features\n",
    "    else:\n",
    "        inputs = (features, test_y)\n",
    "    \n",
    "    dataset = tf.data.Dataset.from_tensor_slices(inputs)\n",
    "    \n",
    "    #Batch size should not be none here.\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    \n",
    "    return dataset\n",
    "\n",
    "\n",
    "((train_x, train_y), (test_x, test_y)) = load_data(TRAIN_URL, TEST_URL)\n",
    "print (train_input_function(train_x, train_y, 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using default config.\n",
      "WARNING:tensorflow:Using temporary folder as model directory: C:\\Users\\USER\\AppData\\Local\\Temp\\tmp809azwzm\n",
      "INFO:tensorflow:Using config: {'_model_dir': 'C:\\\\Users\\\\USER\\\\AppData\\\\Local\\\\Temp\\\\tmp809azwzm', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': None, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x000000000C5009B0>, '_task_type': 'worker', '_task_id': 0, '_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Saving checkpoints for 1 into C:\\Users\\USER\\AppData\\Local\\Temp\\tmp809azwzm\\model.ckpt.\n",
      "INFO:tensorflow:loss = 164.686, step = 1\n",
      "INFO:tensorflow:global_step/sec: 537.602\n",
      "INFO:tensorflow:loss = 24.9764, step = 101 (0.188 sec)\n",
      "INFO:tensorflow:global_step/sec: 689.617\n",
      "INFO:tensorflow:loss = 14.5913, step = 201 (0.145 sec)\n",
      "INFO:tensorflow:global_step/sec: 680.233\n",
      "INFO:tensorflow:loss = 10.1388, step = 301 (0.147 sec)\n",
      "INFO:tensorflow:global_step/sec: 709.18\n",
      "INFO:tensorflow:loss = 6.10183, step = 401 (0.141 sec)\n",
      "INFO:tensorflow:global_step/sec: 704.184\n",
      "INFO:tensorflow:loss = 7.04703, step = 501 (0.142 sec)\n",
      "INFO:tensorflow:global_step/sec: 671.103\n",
      "INFO:tensorflow:loss = 7.21031, step = 601 (0.149 sec)\n",
      "INFO:tensorflow:global_step/sec: 675.638\n",
      "INFO:tensorflow:loss = 4.37742, step = 701 (0.148 sec)\n",
      "INFO:tensorflow:global_step/sec: 680.233\n",
      "INFO:tensorflow:loss = 2.90112, step = 801 (0.147 sec)\n",
      "INFO:tensorflow:global_step/sec: 684.892\n",
      "INFO:tensorflow:loss = 3.70169, step = 901 (0.147 sec)\n",
      "INFO:tensorflow:global_step/sec: 666.629\n",
      "INFO:tensorflow:loss = 5.56609, step = 1001 (0.149 sec)\n",
      "INFO:tensorflow:global_step/sec: 628.895\n",
      "INFO:tensorflow:loss = 8.76456, step = 1101 (0.160 sec)\n",
      "INFO:tensorflow:global_step/sec: 606.026\n",
      "INFO:tensorflow:loss = 3.49396, step = 1201 (0.165 sec)\n",
      "INFO:tensorflow:global_step/sec: 458.689\n",
      "INFO:tensorflow:loss = 5.46284, step = 1301 (0.219 sec)\n",
      "INFO:tensorflow:global_step/sec: 636.907\n",
      "INFO:tensorflow:loss = 3.29806, step = 1401 (0.157 sec)\n",
      "INFO:tensorflow:global_step/sec: 613.462\n",
      "INFO:tensorflow:loss = 3.26224, step = 1501 (0.163 sec)\n",
      "INFO:tensorflow:global_step/sec: 591.682\n",
      "INFO:tensorflow:loss = 5.92606, step = 1601 (0.168 sec)\n",
      "INFO:tensorflow:global_step/sec: 497.484\n",
      "INFO:tensorflow:loss = 7.58293, step = 1701 (0.200 sec)\n",
      "INFO:tensorflow:global_step/sec: 480.741\n",
      "INFO:tensorflow:loss = 5.7587, step = 1801 (0.210 sec)\n",
      "INFO:tensorflow:global_step/sec: 568.15\n",
      "INFO:tensorflow:loss = 5.48319, step = 1901 (0.174 sec)\n",
      "INFO:tensorflow:global_step/sec: 628.895\n",
      "INFO:tensorflow:loss = 2.69997, step = 2001 (0.159 sec)\n",
      "INFO:tensorflow:global_step/sec: 645.124\n",
      "INFO:tensorflow:loss = 5.54118, step = 2101 (0.155 sec)\n",
      "INFO:tensorflow:global_step/sec: 684.892\n",
      "INFO:tensorflow:loss = 4.60916, step = 2201 (0.146 sec)\n",
      "INFO:tensorflow:global_step/sec: 684.893\n",
      "INFO:tensorflow:loss = 2.16235, step = 2301 (0.146 sec)\n",
      "INFO:tensorflow:global_step/sec: 617.25\n",
      "INFO:tensorflow:loss = 2.81909, step = 2401 (0.163 sec)\n",
      "INFO:tensorflow:global_step/sec: 534.729\n",
      "INFO:tensorflow:loss = 2.8929, step = 2501 (0.187 sec)\n",
      "INFO:tensorflow:global_step/sec: 640.989\n",
      "INFO:tensorflow:loss = 7.72696, step = 2601 (0.155 sec)\n",
      "INFO:tensorflow:global_step/sec: 694.404\n",
      "INFO:tensorflow:loss = 5.2235, step = 2701 (0.145 sec)\n",
      "INFO:tensorflow:global_step/sec: 680.233\n",
      "INFO:tensorflow:loss = 2.98528, step = 2801 (0.147 sec)\n",
      "INFO:tensorflow:global_step/sec: 689.617\n",
      "INFO:tensorflow:loss = 6.20902, step = 2901 (0.144 sec)\n",
      "INFO:tensorflow:global_step/sec: 719.383\n",
      "INFO:tensorflow:loss = 4.73494, step = 3001 (0.140 sec)\n",
      "INFO:tensorflow:global_step/sec: 675.638\n",
      "INFO:tensorflow:loss = 5.13219, step = 3101 (0.147 sec)\n",
      "INFO:tensorflow:global_step/sec: 680.232\n",
      "INFO:tensorflow:loss = 6.28219, step = 3201 (0.147 sec)\n",
      "INFO:tensorflow:global_step/sec: 671.103\n",
      "INFO:tensorflow:loss = 5.52217, step = 3301 (0.149 sec)\n",
      "INFO:tensorflow:global_step/sec: 689.617\n",
      "INFO:tensorflow:loss = 6.29811, step = 3401 (0.146 sec)\n",
      "INFO:tensorflow:global_step/sec: 675.636\n",
      "INFO:tensorflow:loss = 4.09051, step = 3501 (0.147 sec)\n",
      "INFO:tensorflow:global_step/sec: 675.638\n",
      "INFO:tensorflow:loss = 4.71903, step = 3601 (0.149 sec)\n",
      "INFO:tensorflow:global_step/sec: 684.892\n",
      "INFO:tensorflow:loss = 4.78074, step = 3701 (0.145 sec)\n",
      "INFO:tensorflow:global_step/sec: 609.721\n",
      "INFO:tensorflow:loss = 5.06634, step = 3801 (0.166 sec)\n",
      "INFO:tensorflow:global_step/sec: 588.202\n",
      "INFO:tensorflow:loss = 5.12861, step = 3901 (0.168 sec)\n",
      "INFO:tensorflow:global_step/sec: 671.102\n",
      "INFO:tensorflow:loss = 2.79068, step = 4001 (0.149 sec)\n",
      "INFO:tensorflow:global_step/sec: 689.616\n",
      "INFO:tensorflow:loss = 5.93525, step = 4101 (0.145 sec)\n",
      "INFO:tensorflow:global_step/sec: 675.638\n",
      "INFO:tensorflow:loss = 5.21694, step = 4201 (0.148 sec)\n",
      "INFO:tensorflow:global_step/sec: 632.875\n",
      "INFO:tensorflow:loss = 4.71308, step = 4301 (0.159 sec)\n",
      "INFO:tensorflow:global_step/sec: 568.149\n",
      "INFO:tensorflow:loss = 5.09045, step = 4401 (0.177 sec)\n",
      "INFO:tensorflow:global_step/sec: 561.766\n",
      "INFO:tensorflow:loss = 5.24225, step = 4501 (0.175 sec)\n",
      "INFO:tensorflow:global_step/sec: 598.768\n",
      "INFO:tensorflow:loss = 2.4843, step = 4601 (0.168 sec)\n",
      "INFO:tensorflow:global_step/sec: 632.875\n",
      "INFO:tensorflow:loss = 6.04088, step = 4701 (0.158 sec)\n",
      "INFO:tensorflow:global_step/sec: 675.636\n",
      "INFO:tensorflow:loss = 2.13911, step = 4801 (0.148 sec)\n",
      "INFO:tensorflow:global_step/sec: 671.103\n",
      "INFO:tensorflow:loss = 2.89007, step = 4901 (0.149 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 5000 into C:\\Users\\USER\\AppData\\Local\\Temp\\tmp809azwzm\\model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 2.83826.\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\tmp809azwzm\n"
     ]
    }
   ],
   "source": [
    "\"\"\"This part of the code deals with using a pre-existing Tensorflow estimator\n",
    "for the purpose of learning from the training set.\"\"\"\n",
    "\n",
    "\n",
    "def train_DNN_classifier(train_x, train_y, batch_size, num_steps):\n",
    "    \"\"\"Function to train a DNN classifier on the iris dataset\"\"\"\n",
    "    \n",
    "    # Step 1: Build the features\n",
    "    # At each step of the loop we are going to add a numeric column to\n",
    "    # the feature set.\n",
    "    feature_columns = []\n",
    "    for key in train_x.keys():\n",
    "        feature_columns.append(tf.feature_column.numeric_column(key = key))\n",
    "    \n",
    "    \n",
    "    # Step 2: Build the DNN model\n",
    "    # * We input the feature_column names we created above.\n",
    "    # * We input the structure of the hidden layers (two hidden layers each of 10 units)\n",
    "    # * We input the number of classes (because the default value = 2)\n",
    "    dnn_classifier = tf.estimator.DNNClassifier(\n",
    "        feature_columns = feature_columns,\n",
    "        hidden_units = [10,10],\n",
    "        n_classes = 3)\n",
    "    \n",
    "    \n",
    "    # Step 3: Train the DNN on the training set provided as parameters\n",
    "    # * Create a dataset out of the train_x and train_y objects\n",
    "    # * train the classifier on the dataset object thus produced\n",
    "    dnn_classifier.train(input_fn=lambda:train_input_function(train_x, train_y, batch_size), steps = num_steps)\n",
    "    \n",
    "    return dnn_classifier\n",
    "\n",
    "trained_classifier = train_DNN_classifier(train_x, train_y, 100, 5000)\n",
    "print (trained_classifier.model_dir)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Starting evaluation at 2018-02-10-12:26:02\n",
      "INFO:tensorflow:Restoring parameters from C:\\Users\\USER\\AppData\\Local\\Temp\\tmp809azwzm\\model.ckpt-5000\n",
      "INFO:tensorflow:Finished evaluation at 2018-02-10-12:26:03\n",
      "INFO:tensorflow:Saving dict for global step 5000: accuracy = 0.966667, average_loss = 0.0578378, global_step = 5000, loss = 1.73514\n",
      "{'accuracy': 0.96666664, 'average_loss': 0.057837836, 'loss': 1.7351351, 'global_step': 5000}\n",
      "Accuracy:  96.6666638851 %\n"
     ]
    }
   ],
   "source": [
    "\"\"\"This part of the code tests the trained DNN classifier on the test set.\"\"\"\n",
    "\n",
    "# Calculating overall test set error\n",
    "batch_size = 100\n",
    "eval_result = trained_classifier.evaluate(\n",
    "    input_fn = lambda:test_input_function(test_x, test_y, batch_size))\n",
    "\n",
    "print (eval_result)\n",
    "print ('Accuracy: ', eval_result['accuracy'] * 100, '%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from C:\\Users\\USER\\AppData\\Local\\Temp\\tmp809azwzm\\model.ckpt-5000\n",
      "\n",
      "Prediction is \"Sentosa\" (99.8%), expected \"Setosa\"\n",
      "\n",
      "Prediction is \"Versicolor\" (100.0%), expected \"Versicolor\"\n",
      "\n",
      "Prediction is \"Virginica\" (99.6%), expected \"Virginica\"\n"
     ]
    }
   ],
   "source": [
    "# Calculating predictions on some inputs\n",
    "expected = ['Setosa', 'Versicolor', 'Virginica']\n",
    "predict_x = {\n",
    "    'SepalLength': [5.1, 5.9, 6.9],\n",
    "    'SepalWidth': [3.3, 3.0, 3.1],\n",
    "    'PetalLength': [1.7, 4.2, 5.4],\n",
    "    'PetalWidth': [0.5, 1.5, 2.1],\n",
    "}\n",
    "\n",
    "predictions = trained_classifier.predict(\n",
    "    input_fn=lambda:test_input_function(predict_x,\n",
    "                                test_y=None,\n",
    "                                batch_size=batch_size))\n",
    "\n",
    "for pred_dict, expec in zip(predictions, expected):\n",
    "    template = ('\\nPrediction is \"{}\" ({:.1f}%), expected \"{}\"')\n",
    "\n",
    "    class_id = pred_dict['class_ids'][0]\n",
    "    probability = pred_dict['probabilities'][class_id]\n",
    "\n",
    "    print(template.format(SPECIES[class_id],\n",
    "                          100 * probability, expec))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using default config.\n",
      "INFO:tensorflow:Using config: {'_model_dir': 'C:\\\\My personal folder\\\\Computer Science Docs\\\\Postgrad(MSc)\\\\Project\\\\Tensorflow tutorial\\\\Iris\\\\model_checkpoints', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': None, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x000000000E8F72E8>, '_task_type': 'worker', '_task_id': 0, '_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Restoring parameters from C:\\My personal folder\\Computer Science Docs\\Postgrad(MSc)\\Project\\Tensorflow tutorial\\Iris\\model_checkpoints\\model.ckpt-10000\n",
      "INFO:tensorflow:Saving checkpoints for 10001 into C:\\My personal folder\\Computer Science Docs\\Postgrad(MSc)\\Project\\Tensorflow tutorial\\Iris\\model_checkpoints\\model.ckpt.\n",
      "INFO:tensorflow:loss = 3.92901, step = 10001\n",
      "INFO:tensorflow:global_step/sec: 518.106\n",
      "INFO:tensorflow:loss = 3.41257, step = 10101 (0.195 sec)\n",
      "INFO:tensorflow:global_step/sec: 636.906\n",
      "INFO:tensorflow:loss = 1.08586, step = 10201 (0.156 sec)\n",
      "INFO:tensorflow:global_step/sec: 640.99\n",
      "INFO:tensorflow:loss = 3.80971, step = 10301 (0.156 sec)\n",
      "INFO:tensorflow:global_step/sec: 632.875\n",
      "INFO:tensorflow:loss = 3.0124, step = 10401 (0.159 sec)\n",
      "INFO:tensorflow:global_step/sec: 529.069\n",
      "INFO:tensorflow:loss = 3.29762, step = 10501 (0.191 sec)\n",
      "INFO:tensorflow:global_step/sec: 529.071\n",
      "INFO:tensorflow:loss = 3.73582, step = 10601 (0.187 sec)\n",
      "INFO:tensorflow:global_step/sec: 617.249\n",
      "INFO:tensorflow:loss = 2.88927, step = 10701 (0.161 sec)\n",
      "INFO:tensorflow:global_step/sec: 640.989\n",
      "INFO:tensorflow:loss = 3.67862, step = 10801 (0.156 sec)\n",
      "INFO:tensorflow:global_step/sec: 645.125\n",
      "INFO:tensorflow:loss = 1.91564, step = 10901 (0.155 sec)\n",
      "INFO:tensorflow:global_step/sec: 624.963\n",
      "INFO:tensorflow:loss = 0.950629, step = 11001 (0.162 sec)\n",
      "INFO:tensorflow:global_step/sec: 543.447\n",
      "INFO:tensorflow:loss = 3.81252, step = 11101 (0.182 sec)\n",
      "INFO:tensorflow:global_step/sec: 645.124\n",
      "INFO:tensorflow:loss = 3.50428, step = 11201 (0.156 sec)\n",
      "INFO:tensorflow:global_step/sec: 666.629\n",
      "INFO:tensorflow:loss = 3.04931, step = 11301 (0.150 sec)\n",
      "INFO:tensorflow:global_step/sec: 671.103\n",
      "INFO:tensorflow:loss = 2.53444, step = 11401 (0.148 sec)\n",
      "INFO:tensorflow:global_step/sec: 694.405\n",
      "INFO:tensorflow:loss = 3.37736, step = 11501 (0.145 sec)\n",
      "INFO:tensorflow:global_step/sec: 613.462\n",
      "INFO:tensorflow:loss = 2.95344, step = 11601 (0.162 sec)\n",
      "INFO:tensorflow:global_step/sec: 578.002\n",
      "INFO:tensorflow:loss = 4.55162, step = 11701 (0.174 sec)\n",
      "INFO:tensorflow:global_step/sec: 617.248\n",
      "INFO:tensorflow:loss = 2.78553, step = 11801 (0.161 sec)\n",
      "INFO:tensorflow:global_step/sec: 666.629\n",
      "INFO:tensorflow:loss = 1.19227, step = 11901 (0.150 sec)\n",
      "INFO:tensorflow:global_step/sec: 671.103\n",
      "INFO:tensorflow:loss = 4.16512, step = 12001 (0.149 sec)\n",
      "INFO:tensorflow:global_step/sec: 617.249\n",
      "INFO:tensorflow:loss = 3.001, step = 12101 (0.163 sec)\n",
      "INFO:tensorflow:global_step/sec: 588.201\n",
      "INFO:tensorflow:loss = 3.79433, step = 12201 (0.170 sec)\n",
      "INFO:tensorflow:global_step/sec: 636.907\n",
      "INFO:tensorflow:loss = 3.68234, step = 12301 (0.157 sec)\n",
      "INFO:tensorflow:global_step/sec: 653.558\n",
      "INFO:tensorflow:loss = 3.57402, step = 12401 (0.152 sec)\n",
      "INFO:tensorflow:global_step/sec: 657.857\n",
      "INFO:tensorflow:loss = 3.11195, step = 12501 (0.152 sec)\n",
      "INFO:tensorflow:global_step/sec: 684.892\n",
      "INFO:tensorflow:loss = 0.645834, step = 12601 (0.146 sec)\n",
      "INFO:tensorflow:global_step/sec: 689.617\n",
      "INFO:tensorflow:loss = 3.39026, step = 12701 (0.145 sec)\n",
      "INFO:tensorflow:global_step/sec: 666.628\n",
      "INFO:tensorflow:loss = 0.817069, step = 12801 (0.151 sec)\n",
      "INFO:tensorflow:global_step/sec: 584.762\n",
      "INFO:tensorflow:loss = 3.57652, step = 12901 (0.171 sec)\n",
      "INFO:tensorflow:global_step/sec: 588.201\n",
      "INFO:tensorflow:loss = 3.31147, step = 13001 (0.170 sec)\n",
      "INFO:tensorflow:global_step/sec: 636.907\n",
      "INFO:tensorflow:loss = 3.25823, step = 13101 (0.157 sec)\n",
      "INFO:tensorflow:global_step/sec: 657.857\n",
      "INFO:tensorflow:loss = 5.27911, step = 13201 (0.151 sec)\n",
      "INFO:tensorflow:global_step/sec: 666.629\n",
      "INFO:tensorflow:loss = 3.38154, step = 13301 (0.150 sec)\n",
      "INFO:tensorflow:global_step/sec: 689.614\n",
      "INFO:tensorflow:loss = 3.10222, step = 13401 (0.145 sec)\n",
      "INFO:tensorflow:global_step/sec: 694.405\n",
      "INFO:tensorflow:loss = 2.6675, step = 13501 (0.144 sec)\n",
      "INFO:tensorflow:global_step/sec: 632.875\n",
      "INFO:tensorflow:loss = 2.73027, step = 13601 (0.161 sec)\n",
      "INFO:tensorflow:global_step/sec: 578.002\n",
      "INFO:tensorflow:loss = 3.82916, step = 13701 (0.171 sec)\n",
      "INFO:tensorflow:global_step/sec: 632.876\n",
      "INFO:tensorflow:loss = 2.30124, step = 13801 (0.158 sec)\n",
      "INFO:tensorflow:global_step/sec: 653.557\n",
      "INFO:tensorflow:loss = 3.56767, step = 13901 (0.153 sec)\n",
      "INFO:tensorflow:global_step/sec: 689.616\n",
      "INFO:tensorflow:loss = 2.81482, step = 14001 (0.145 sec)\n",
      "INFO:tensorflow:global_step/sec: 645.125\n",
      "INFO:tensorflow:loss = 4.98026, step = 14101 (0.154 sec)\n",
      "INFO:tensorflow:global_step/sec: 584.762\n",
      "INFO:tensorflow:loss = 3.53049, step = 14201 (0.172 sec)\n",
      "INFO:tensorflow:global_step/sec: 666.628\n",
      "INFO:tensorflow:loss = 3.62428, step = 14301 (0.149 sec)\n",
      "INFO:tensorflow:global_step/sec: 714.245\n",
      "INFO:tensorflow:loss = 2.77265, step = 14401 (0.140 sec)\n",
      "INFO:tensorflow:global_step/sec: 645.124\n",
      "INFO:tensorflow:loss = 3.12232, step = 14501 (0.155 sec)\n",
      "INFO:tensorflow:global_step/sec: 598.77\n",
      "INFO:tensorflow:loss = 1.93891, step = 14601 (0.168 sec)\n",
      "INFO:tensorflow:global_step/sec: 591.681\n",
      "INFO:tensorflow:loss = 3.3312, step = 14701 (0.169 sec)\n",
      "INFO:tensorflow:global_step/sec: 624.965\n",
      "INFO:tensorflow:loss = 1.6154, step = 14801 (0.160 sec)\n",
      "INFO:tensorflow:global_step/sec: 675.636\n",
      "INFO:tensorflow:loss = 2.0296, step = 14901 (0.148 sec)\n",
      "INFO:tensorflow:global_step/sec: 684.893\n",
      "INFO:tensorflow:loss = 5.30681, step = 15001 (0.145 sec)\n",
      "INFO:tensorflow:global_step/sec: 666.628\n",
      "INFO:tensorflow:loss = 2.78375, step = 15101 (0.151 sec)\n",
      "INFO:tensorflow:global_step/sec: 645.124\n",
      "INFO:tensorflow:loss = 3.29534, step = 15201 (0.155 sec)\n",
      "INFO:tensorflow:global_step/sec: 649.313\n",
      "INFO:tensorflow:loss = 0.944162, step = 15301 (0.153 sec)\n",
      "INFO:tensorflow:global_step/sec: 645.126\n",
      "INFO:tensorflow:loss = 3.21156, step = 15401 (0.156 sec)\n",
      "INFO:tensorflow:global_step/sec: 595.204\n",
      "INFO:tensorflow:loss = 5.31998, step = 15501 (0.169 sec)\n",
      "INFO:tensorflow:global_step/sec: 632.876\n",
      "INFO:tensorflow:loss = 5.13626, step = 15601 (0.157 sec)\n",
      "INFO:tensorflow:global_step/sec: 704.184\n",
      "INFO:tensorflow:loss = 3.26475, step = 15701 (0.142 sec)\n",
      "INFO:tensorflow:global_step/sec: 574.679\n",
      "INFO:tensorflow:loss = 2.99071, step = 15801 (0.175 sec)\n",
      "INFO:tensorflow:global_step/sec: 581.363\n",
      "INFO:tensorflow:loss = 4.81767, step = 15901 (0.170 sec)\n",
      "INFO:tensorflow:global_step/sec: 624.964\n",
      "INFO:tensorflow:loss = 3.16353, step = 16001 (0.161 sec)\n",
      "INFO:tensorflow:global_step/sec: 584.762\n",
      "INFO:tensorflow:loss = 3.97132, step = 16101 (0.171 sec)\n",
      "INFO:tensorflow:global_step/sec: 602.375\n",
      "INFO:tensorflow:loss = 5.2319, step = 16201 (0.166 sec)\n",
      "INFO:tensorflow:global_step/sec: 680.233\n",
      "INFO:tensorflow:loss = 3.09529, step = 16301 (0.146 sec)\n",
      "INFO:tensorflow:global_step/sec: 724.596\n",
      "INFO:tensorflow:loss = 5.00007, step = 16401 (0.138 sec)\n",
      "INFO:tensorflow:global_step/sec: 666.629\n",
      "INFO:tensorflow:loss = 2.96718, step = 16501 (0.150 sec)\n",
      "INFO:tensorflow:global_step/sec: 675.636\n",
      "INFO:tensorflow:loss = 3.36748, step = 16601 (0.148 sec)\n",
      "INFO:tensorflow:global_step/sec: 694.405\n",
      "INFO:tensorflow:loss = 3.577, step = 16701 (0.145 sec)\n",
      "INFO:tensorflow:global_step/sec: 680.234\n",
      "INFO:tensorflow:loss = 1.08563, step = 16801 (0.146 sec)\n",
      "INFO:tensorflow:global_step/sec: 675.635\n",
      "INFO:tensorflow:loss = 3.34069, step = 16901 (0.149 sec)\n",
      "INFO:tensorflow:global_step/sec: 694.405\n",
      "INFO:tensorflow:loss = 4.07165, step = 17001 (0.144 sec)\n",
      "INFO:tensorflow:global_step/sec: 680.233\n",
      "INFO:tensorflow:loss = 1.84542, step = 17101 (0.146 sec)\n",
      "INFO:tensorflow:global_step/sec: 680.233\n",
      "INFO:tensorflow:loss = 2.82825, step = 17201 (0.147 sec)\n",
      "INFO:tensorflow:global_step/sec: 657.858\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 2.29234, step = 17301 (0.152 sec)\n",
      "INFO:tensorflow:global_step/sec: 653.558\n",
      "INFO:tensorflow:loss = 5.78773, step = 17401 (0.154 sec)\n",
      "INFO:tensorflow:global_step/sec: 671.102\n",
      "INFO:tensorflow:loss = 1.72691, step = 17501 (0.149 sec)\n",
      "INFO:tensorflow:global_step/sec: 666.629\n",
      "INFO:tensorflow:loss = 4.18541, step = 17601 (0.149 sec)\n",
      "INFO:tensorflow:global_step/sec: 671.103\n",
      "INFO:tensorflow:loss = 2.7016, step = 17701 (0.149 sec)\n",
      "INFO:tensorflow:global_step/sec: 704.184\n",
      "INFO:tensorflow:loss = 1.69407, step = 17801 (0.142 sec)\n",
      "INFO:tensorflow:global_step/sec: 675.638\n",
      "INFO:tensorflow:loss = 4.12515, step = 17901 (0.149 sec)\n",
      "INFO:tensorflow:global_step/sec: 666.628\n",
      "INFO:tensorflow:loss = 1.35704, step = 18001 (0.149 sec)\n",
      "INFO:tensorflow:global_step/sec: 675.638\n",
      "INFO:tensorflow:loss = 2.91923, step = 18101 (0.149 sec)\n",
      "INFO:tensorflow:global_step/sec: 684.892\n",
      "INFO:tensorflow:loss = 1.73848, step = 18201 (0.146 sec)\n",
      "INFO:tensorflow:global_step/sec: 640.989\n",
      "INFO:tensorflow:loss = 3.44349, step = 18301 (0.155 sec)\n",
      "INFO:tensorflow:global_step/sec: 704.185\n",
      "INFO:tensorflow:loss = 3.01152, step = 18401 (0.142 sec)\n",
      "INFO:tensorflow:global_step/sec: 709.18\n",
      "INFO:tensorflow:loss = 2.98937, step = 18501 (0.141 sec)\n",
      "INFO:tensorflow:global_step/sec: 714.245\n",
      "INFO:tensorflow:loss = 2.69646, step = 18601 (0.140 sec)\n",
      "INFO:tensorflow:global_step/sec: 684.892\n",
      "INFO:tensorflow:loss = 2.98861, step = 18701 (0.147 sec)\n",
      "INFO:tensorflow:global_step/sec: 649.314\n",
      "INFO:tensorflow:loss = 3.12528, step = 18801 (0.153 sec)\n",
      "INFO:tensorflow:global_step/sec: 684.892\n",
      "INFO:tensorflow:loss = 1.7041, step = 18901 (0.146 sec)\n",
      "INFO:tensorflow:global_step/sec: 684.893\n",
      "INFO:tensorflow:loss = 1.18044, step = 19001 (0.146 sec)\n",
      "INFO:tensorflow:global_step/sec: 709.179\n",
      "INFO:tensorflow:loss = 2.59152, step = 19101 (0.142 sec)\n",
      "INFO:tensorflow:global_step/sec: 699.26\n",
      "INFO:tensorflow:loss = 2.86346, step = 19201 (0.142 sec)\n",
      "INFO:tensorflow:global_step/sec: 735.252\n",
      "INFO:tensorflow:loss = 3.08719, step = 19301 (0.136 sec)\n",
      "INFO:tensorflow:global_step/sec: 709.179\n",
      "INFO:tensorflow:loss = 2.1734, step = 19401 (0.142 sec)\n",
      "INFO:tensorflow:global_step/sec: 704.185\n",
      "INFO:tensorflow:loss = 2.42888, step = 19501 (0.141 sec)\n",
      "INFO:tensorflow:global_step/sec: 709.179\n",
      "INFO:tensorflow:loss = 2.81124, step = 19601 (0.142 sec)\n",
      "INFO:tensorflow:global_step/sec: 704.185\n",
      "INFO:tensorflow:loss = 2.41425, step = 19701 (0.141 sec)\n",
      "INFO:tensorflow:global_step/sec: 709.18\n",
      "INFO:tensorflow:loss = 4.11432, step = 19801 (0.142 sec)\n",
      "INFO:tensorflow:global_step/sec: 704.185\n",
      "INFO:tensorflow:loss = 2.66945, step = 19901 (0.142 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 20000 into C:\\My personal folder\\Computer Science Docs\\Postgrad(MSc)\\Project\\Tensorflow tutorial\\Iris\\model_checkpoints\\model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 2.55409.\n"
     ]
    }
   ],
   "source": [
    "# This piece of code deals with a DNNClassifier with checkpoints\n",
    "def train_DNN_classifier_with_checkpoints(train_x, train_y, batch_size, num_steps):\n",
    "    \"\"\"Function to train a DNN classifier on the iris dataset with configured checkpoints\"\"\"\n",
    "    \n",
    "    # Step 1: Build the features\n",
    "    # At each step of the loop we are going to add a numeric column to\n",
    "    # the feature set.\n",
    "    feature_columns = []\n",
    "    for key in train_x.keys():\n",
    "        feature_columns.append(tf.feature_column.numeric_column(key = key))\n",
    "    \n",
    "    \n",
    "    # Step 2: Build the DNN model\n",
    "    # * We input the feature_column names we created above.\n",
    "    # * We input the structure of the hidden layers (two hidden layers each of 10 units)\n",
    "    # * We input the number of classes (because the default value = 2)\n",
    "    dnn_classifier = tf.estimator.DNNClassifier(\n",
    "        feature_columns = feature_columns,\n",
    "        hidden_units = [10,10],\n",
    "        model_dir = 'C:\\My personal folder\\Computer Science Docs\\Postgrad(MSc)\\Project\\Tensorflow tutorial\\Iris\\model_checkpoints',\n",
    "        n_classes = 3)\n",
    "    \n",
    "    \n",
    "    # Step 3: Train the DNN on the training set provided as parameters\n",
    "    # * Create a dataset out of the train_x and train_y objects\n",
    "    # * train the classifier on the dataset object thus produced\n",
    "    dnn_classifier.train(input_fn=lambda:train_input_function(train_x, train_y, batch_size), steps = num_steps)\n",
    "    \n",
    "    return dnn_classifier\n",
    "\n",
    "\n",
    "dnn_classifier = train_DNN_classifier_with_checkpoints(train_x, train_y, 100, 10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
