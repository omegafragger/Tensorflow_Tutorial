{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<BatchDataset shapes: ({SepalLength: (?,), SepalWidth: (?,), PetalLength: (?,), PetalWidth: (?,)}, (?,)), types: ({SepalLength: tf.float64, SepalWidth: tf.float64, PetalLength: tf.float64, PetalWidth: tf.float64}, tf.int64)>\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Tensorflow tutorial for predicting on the IRIS dataset\n",
    "Also contains general methods which will be useful for:\n",
    "\n",
    "* Downloading training and test sets given their URLs.\n",
    "* \"\"\"\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "TRAIN_URL = \"http://download.tensorflow.org/data/iris_training.csv\"\n",
    "TEST_URL = \"http://download.tensorflow.org/data/iris_test.csv\"\n",
    "\n",
    "CSV_COLUMN_NAMES = ['SepalLength', 'SepalWidth',\n",
    "                    'PetalLength', 'PetalWidth', 'Species']\n",
    "SPECIES = ['Sentosa', 'Versicolor', 'Virginica']\n",
    "\n",
    "\n",
    "\"\"\"The following two methods contain functionality for loading and reading\n",
    "data into the training and test sets.\"\"\"\n",
    "\n",
    "def download_if_not_in_cache(train_url, test_url, train_file_name, test_file_name):\n",
    "    \"\"\"Method to download a training and test set given their URLs.\n",
    "    * The method checks if the file already exists in the cache.\n",
    "    * If not it downloads and stores the file in the cache.\n",
    "    * Finally, it returns the path to the file which has been downloaded.\"\"\"\n",
    "    \n",
    "    train_file_path = tf.keras.utils.get_file(train_file_name, train_url)\n",
    "    test_file_path = tf.keras.utils.get_file(test_file_name, test_url)\n",
    "    \n",
    "    return train_file_path, test_file_path\n",
    "\n",
    "\n",
    "def load_data(train_url, test_url):\n",
    "    train_path, test_path = download_if_not_in_cache(train_url, test_url, train_url.split('/')[-1], test_url.split('/')[-1])\n",
    "\n",
    "    train = pd.read_csv(train_path, names = CSV_COLUMN_NAMES, header=0)\n",
    "    train_x, train_y = train, train.pop('Species')\n",
    "    \n",
    "    test = pd.read_csv(test_path, names = CSV_COLUMN_NAMES, header=0)\n",
    "    test_x, test_y = test, test.pop('Species')\n",
    "    \n",
    "    return ((train_x, train_y), (test_x, test_y))\n",
    "\n",
    "\n",
    "def train_input_function(train_x, train_y, batch_size):\n",
    "    \"\"\"Function to produce a dataset type object from the training data produced\n",
    "    by the load_data function above. The dataset type object produced by this method\n",
    "    can be passed to the 'train' function of the estimator (like DNNClassifier)\"\"\"\n",
    "    \n",
    "    # train_x is a DataFrame type object and we are converting it into a dictionary\n",
    "    # train_y is a Series type object <class 'pandas.core.series.Series'>\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((dict(train_x), train_y))\n",
    "    \n",
    "    # Shuffling, Repeating and Batching the dataset\n",
    "    dataset = dataset.shuffle(1000).repeat().batch(batch_size)\n",
    "    \n",
    "    return dataset\n",
    "\n",
    "\n",
    "def test_input_function(test_x, test_y, batch_size):\n",
    "    \"\"\"Similar function as above but for the test data set\"\"\"\n",
    "    features = dict(test_x)\n",
    "    \n",
    "    # This case may turn up when we are trying to make predictions\n",
    "    # on single points using a trained classifier\n",
    "    if test_y is None:\n",
    "        inputs = features\n",
    "    else:\n",
    "        inputs = (features, test_y)\n",
    "    \n",
    "    dataset = tf.data.Dataset.from_tensor_slices(inputs)\n",
    "    \n",
    "    #Batch size should not be none here.\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    \n",
    "    return dataset\n",
    "\n",
    "\n",
    "((train_x, train_y), (test_x, test_y)) = load_data(TRAIN_URL, TEST_URL)\n",
    "print (train_input_function(train_x, train_y, 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using default config.\n",
      "WARNING:tensorflow:Using temporary folder as model directory: C:\\Users\\USER\\AppData\\Local\\Temp\\tmp91o93vb5\n",
      "INFO:tensorflow:Using config: {'_model_dir': 'C:\\\\Users\\\\USER\\\\AppData\\\\Local\\\\Temp\\\\tmp91o93vb5', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': None, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x000000000C72C198>, '_task_type': 'worker', '_task_id': 0, '_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Saving checkpoints for 1 into C:\\Users\\USER\\AppData\\Local\\Temp\\tmp91o93vb5\\model.ckpt.\n",
      "INFO:tensorflow:loss = 152.044, step = 1\n",
      "INFO:tensorflow:global_step/sec: 746.227\n",
      "INFO:tensorflow:loss = 24.6768, step = 101 (0.135 sec)\n",
      "INFO:tensorflow:global_step/sec: 917.377\n",
      "INFO:tensorflow:loss = 15.4043, step = 201 (0.109 sec)\n",
      "INFO:tensorflow:global_step/sec: 934.526\n",
      "INFO:tensorflow:loss = 7.65895, step = 301 (0.107 sec)\n",
      "INFO:tensorflow:global_step/sec: 952.327\n",
      "INFO:tensorflow:loss = 6.26332, step = 401 (0.106 sec)\n",
      "INFO:tensorflow:global_step/sec: 826.399\n",
      "INFO:tensorflow:loss = 4.87043, step = 501 (0.120 sec)\n",
      "INFO:tensorflow:global_step/sec: 1041.61\n",
      "INFO:tensorflow:loss = 7.8279, step = 601 (0.096 sec)\n",
      "INFO:tensorflow:global_step/sec: 1020.35\n",
      "INFO:tensorflow:loss = 7.75406, step = 701 (0.099 sec)\n",
      "INFO:tensorflow:global_step/sec: 840.288\n",
      "INFO:tensorflow:loss = 5.08396, step = 801 (0.119 sec)\n",
      "INFO:tensorflow:global_step/sec: 909.041\n",
      "INFO:tensorflow:loss = 6.62289, step = 901 (0.109 sec)\n",
      "INFO:tensorflow:global_step/sec: 999.941\n",
      "INFO:tensorflow:loss = 3.74479, step = 1001 (0.101 sec)\n",
      "INFO:tensorflow:global_step/sec: 980.335\n",
      "INFO:tensorflow:loss = 6.64145, step = 1101 (0.101 sec)\n",
      "INFO:tensorflow:global_step/sec: 999.944\n",
      "INFO:tensorflow:loss = 5.07746, step = 1201 (0.100 sec)\n",
      "INFO:tensorflow:global_step/sec: 1041.61\n",
      "INFO:tensorflow:loss = 6.31844, step = 1301 (0.096 sec)\n",
      "INFO:tensorflow:global_step/sec: 1052.57\n",
      "INFO:tensorflow:loss = 5.46087, step = 1401 (0.096 sec)\n",
      "INFO:tensorflow:global_step/sec: 970.818\n",
      "INFO:tensorflow:loss = 2.67635, step = 1501 (0.103 sec)\n",
      "INFO:tensorflow:global_step/sec: 854.651\n",
      "INFO:tensorflow:loss = 5.52884, step = 1601 (0.116 sec)\n",
      "INFO:tensorflow:global_step/sec: 900.852\n",
      "INFO:tensorflow:loss = 4.38742, step = 1701 (0.112 sec)\n",
      "INFO:tensorflow:global_step/sec: 909.039\n",
      "INFO:tensorflow:loss = 5.5721, step = 1801 (0.110 sec)\n",
      "INFO:tensorflow:global_step/sec: 925.87\n",
      "INFO:tensorflow:loss = 4.51624, step = 1901 (0.107 sec)\n",
      "INFO:tensorflow:global_step/sec: 1052.57\n",
      "INFO:tensorflow:loss = 5.30178, step = 2001 (0.095 sec)\n",
      "INFO:tensorflow:global_step/sec: 1010.04\n",
      "INFO:tensorflow:loss = 4.97757, step = 2101 (0.099 sec)\n",
      "INFO:tensorflow:global_step/sec: 877.145\n",
      "INFO:tensorflow:loss = 5.16951, step = 2201 (0.116 sec)\n",
      "INFO:tensorflow:global_step/sec: 892.808\n",
      "INFO:tensorflow:loss = 4.58782, step = 2301 (0.111 sec)\n",
      "INFO:tensorflow:global_step/sec: 917.377\n",
      "INFO:tensorflow:loss = 4.97462, step = 2401 (0.109 sec)\n",
      "INFO:tensorflow:global_step/sec: 900.848\n",
      "INFO:tensorflow:loss = 2.24291, step = 2501 (0.110 sec)\n",
      "INFO:tensorflow:global_step/sec: 892.806\n",
      "INFO:tensorflow:loss = 3.91082, step = 2601 (0.114 sec)\n",
      "INFO:tensorflow:global_step/sec: 900.85\n",
      "INFO:tensorflow:loss = 4.87214, step = 2701 (0.109 sec)\n",
      "INFO:tensorflow:global_step/sec: 925.874\n",
      "INFO:tensorflow:loss = 1.03373, step = 2801 (0.110 sec)\n",
      "INFO:tensorflow:global_step/sec: 909.039\n",
      "INFO:tensorflow:loss = 5.00933, step = 2901 (0.108 sec)\n",
      "INFO:tensorflow:global_step/sec: 819.624\n",
      "INFO:tensorflow:loss = 5.11467, step = 3001 (0.125 sec)\n",
      "INFO:tensorflow:global_step/sec: 806.406\n",
      "INFO:tensorflow:loss = 4.97572, step = 3101 (0.122 sec)\n",
      "INFO:tensorflow:global_step/sec: 763.314\n",
      "INFO:tensorflow:loss = 6.10894, step = 3201 (0.131 sec)\n",
      "INFO:tensorflow:global_step/sec: 877.143\n",
      "INFO:tensorflow:loss = 4.95624, step = 3301 (0.113 sec)\n",
      "INFO:tensorflow:global_step/sec: 961.485\n",
      "INFO:tensorflow:loss = 4.87108, step = 3401 (0.106 sec)\n",
      "INFO:tensorflow:global_step/sec: 934.524\n",
      "INFO:tensorflow:loss = 4.38029, step = 3501 (0.105 sec)\n",
      "INFO:tensorflow:global_step/sec: 1041.61\n",
      "INFO:tensorflow:loss = 4.0273, step = 3601 (0.097 sec)\n",
      "INFO:tensorflow:global_step/sec: 869.515\n",
      "INFO:tensorflow:loss = 6.59558, step = 3701 (0.115 sec)\n",
      "INFO:tensorflow:global_step/sec: 1030.87\n",
      "INFO:tensorflow:loss = 2.61573, step = 3801 (0.096 sec)\n",
      "INFO:tensorflow:global_step/sec: 1041.61\n",
      "INFO:tensorflow:loss = 3.86091, step = 3901 (0.096 sec)\n",
      "INFO:tensorflow:global_step/sec: 1052.57\n",
      "INFO:tensorflow:loss = 4.16333, step = 4001 (0.095 sec)\n",
      "INFO:tensorflow:global_step/sec: 1010.04\n",
      "INFO:tensorflow:loss = 2.09437, step = 4101 (0.099 sec)\n",
      "INFO:tensorflow:global_step/sec: 1052.57\n",
      "INFO:tensorflow:loss = 4.61942, step = 4201 (0.095 sec)\n",
      "INFO:tensorflow:global_step/sec: 1052.57\n",
      "INFO:tensorflow:loss = 5.07869, step = 4301 (0.095 sec)\n",
      "INFO:tensorflow:global_step/sec: 1052.57\n",
      "INFO:tensorflow:loss = 3.85598, step = 4401 (0.095 sec)\n",
      "INFO:tensorflow:global_step/sec: 1030.87\n",
      "INFO:tensorflow:loss = 2.22022, step = 4501 (0.097 sec)\n",
      "INFO:tensorflow:global_step/sec: 1063.77\n",
      "INFO:tensorflow:loss = 3.80366, step = 4601 (0.094 sec)\n",
      "INFO:tensorflow:global_step/sec: 1063.77\n",
      "INFO:tensorflow:loss = 3.6599, step = 4701 (0.094 sec)\n",
      "INFO:tensorflow:global_step/sec: 1041.61\n",
      "INFO:tensorflow:loss = 4.63515, step = 4801 (0.097 sec)\n",
      "INFO:tensorflow:global_step/sec: 1063.77\n",
      "INFO:tensorflow:loss = 6.86546, step = 4901 (0.093 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 5000 into C:\\Users\\USER\\AppData\\Local\\Temp\\tmp91o93vb5\\model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 8.06799.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"This part of the code deals with using a pre-existing Tensorflow estimator\n",
    "for the purpose of learning from the training set.\"\"\"\n",
    "\n",
    "\n",
    "def train_DNN_classifier(train_x, train_y, batch_size, num_steps):\n",
    "    \"\"\"Function to train a DNN classifier on the iris dataset\"\"\"\n",
    "    \n",
    "    # Step 1: Build the features\n",
    "    # At each step of the loop we are going to add a numeric column to\n",
    "    # the feature set.\n",
    "    feature_columns = []\n",
    "    for key in train_x.keys():\n",
    "        feature_columns.append(tf.feature_column.numeric_column(key = key))\n",
    "    \n",
    "    \n",
    "    # Step 2: Build the DNN model\n",
    "    # * We input the feature_column names we created above.\n",
    "    # * We input the structure of the hidden layers (two hidden layers each of 10 units)\n",
    "    # * We input the number of classes (because the default value = 2)\n",
    "    dnn_classifier = tf.estimator.DNNClassifier(\n",
    "        feature_columns = feature_columns,\n",
    "        hidden_units = [10,10],\n",
    "        n_classes = 3)\n",
    "    \n",
    "    \n",
    "    # Step 3: Train the DNN on the training set provided as parameters\n",
    "    # * Create a dataset out of the train_x and train_y objects\n",
    "    # * train the classifier on the dataset object thus produced\n",
    "    dnn_classifier.train(input_fn=lambda:train_input_function(train_x, train_y, batch_size), steps = num_steps)\n",
    "    \n",
    "    return dnn_classifier\n",
    "\n",
    "trained_classifier = train_DNN_classifier(train_x, train_y, 100, 5000)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Starting evaluation at 2018-02-09-17:41:16\n",
      "INFO:tensorflow:Restoring parameters from C:\\Users\\USER\\AppData\\Local\\Temp\\tmp91o93vb5\\model.ckpt-5000\n",
      "INFO:tensorflow:Finished evaluation at 2018-02-09-17:41:16\n",
      "INFO:tensorflow:Saving dict for global step 5000: accuracy = 0.966667, average_loss = 0.0749384, global_step = 5000, loss = 2.24815\n",
      "{'accuracy': 0.96666664, 'average_loss': 0.074938416, 'loss': 2.2481525, 'global_step': 5000}\n",
      "Accuracy:  96.6666638851 %\n"
     ]
    }
   ],
   "source": [
    "\"\"\"This part of the code tests the trained DNN classifier on the test set.\"\"\"\n",
    "\n",
    "# Calculating overall test set error\n",
    "batch_size = 100\n",
    "eval_result = trained_classifier.evaluate(\n",
    "    input_fn = lambda:test_input_function(test_x, test_y, batch_size))\n",
    "\n",
    "print (eval_result)\n",
    "print ('Accuracy: ', eval_result['accuracy'] * 100, '%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from C:\\Users\\USER\\AppData\\Local\\Temp\\tmp91o93vb5\\model.ckpt-5000\n",
      "\n",
      "Prediction is \"Sentosa\" (100.0%), expected \"Setosa\"\n",
      "\n",
      "Prediction is \"Versicolor\" (100.0%), expected \"Versicolor\"\n",
      "\n",
      "Prediction is \"Virginica\" (99.5%), expected \"Virginica\"\n"
     ]
    }
   ],
   "source": [
    "# Calculating predictions on some inputs\n",
    "expected = ['Setosa', 'Versicolor', 'Virginica']\n",
    "predict_x = {\n",
    "    'SepalLength': [5.1, 5.9, 6.9],\n",
    "    'SepalWidth': [3.3, 3.0, 3.1],\n",
    "    'PetalLength': [1.7, 4.2, 5.4],\n",
    "    'PetalWidth': [0.5, 1.5, 2.1],\n",
    "}\n",
    "\n",
    "predictions = trained_classifier.predict(\n",
    "    input_fn=lambda:test_input_function(predict_x,\n",
    "                                test_y=None,\n",
    "                                batch_size=batch_size))\n",
    "\n",
    "for pred_dict, expec in zip(predictions, expected):\n",
    "    template = ('\\nPrediction is \"{}\" ({:.1f}%), expected \"{}\"')\n",
    "\n",
    "    class_id = pred_dict['class_ids'][0]\n",
    "    probability = pred_dict['probabilities'][class_id]\n",
    "\n",
    "    print(template.format(SPECIES[class_id],\n",
    "                          100 * probability, expec))\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
